# Awesome-Video-Reasoning-Landscape [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<!-- markdownlint-disable MD033 -->
## The Landscape of Video Reasoning: Tasks, Paradigms and Benchmarks‚Äî An Open-Source Survey

## Overview
This Awesome list systematically curates and tracks the latest progress in **Video Reasoning**, covering diverse modalities, tasks, and modeling paradigms. Rather than focusing on a single line of research, we organize the landscape from multiple complementary perspectives. Following the emerging taxonomy of the field, current works are grouped into four major paradigms:

- üóíÔ∏è **CoT-based Video Reasoning** ‚Äî language-centric, chain-of-thought reasoning with Video-LMMs  
- üïπÔ∏è **CoF-based Video Reasoning** ‚Äî vision-centric reasoning grounded in world models or video generation  
- üåà **Interleaved Video Reasoning** ‚Äî unified models that integrate multimodal interaction and iterative inference  
- üîÅ **Streaming Video Reasoning** ‚Äî continuous, low-latency reasoning over long or unbounded video streams with online perception and incremental state updates.
  
We additionally maintain a dedicated **Benchmark** section that summarizes datasets, evaluation settings, and standardized tasks to support fair comparison across paradigms.

This repository aims to provide a structured, up-to-date, and open-source overview of the evolving landscape of video reasoning.  
**Contributions and PRs are warmly welcome ‚Äî preferably in reverse chronological order (newest first)** to keep the list fresh and easy to browse.





## Table of Contents
- [Awesome-Video-Reasoning-Landscape](#awesome-video-reasoning-landscape-)
  - [üìë Task Definition](#-task-definition)
  - [üòé Paradigms](#-paradigms)
    - [üóíÔ∏è CoT-based Video Reasoning](#Ô∏è-cot-based-video-reasoning)
    - [üïπÔ∏è CoF-based Video Reasoning](#Ô∏è-cof-based-video-reasoning)
    - [üåà Interleaved Video Reasoning](#-interleaved-video-reasoning)
    - [üîÅ Streaming Video Reasoning](#-streaming-video-reasoning)
  - [‚ú®Ô∏è Benchmarks](#%EF%B8%8F-benchmarks)
  - [‚úà Related Surveys](#-related-survey)
  - [üåü Star History](#-star-history)
  - [‚ô•Ô∏è Contributors](#Ô∏è-contributors)

<!-- <small><i><a href='http://eCoTrust-canada.github.io/markdown-toc/'>`Table of contents generated with markdown-toc`</a></i></small> -->

## üìë Task Definition
TBD

## üòé Paradigms

<!-- Á¨¶Âè∑:
‚àö ‚úì
x ‚úó
         arxiv: https://img.shields.io/badge/arXiv-2410.12109-b31b1b.svg?style=flat-square
         conference: https://img.shields.io/badge/CVPR-2024-blue.svg?style=flat-square
         huggingface checkpoint:![hf_checkpoint](https://img.shields.io/badge/ü§ó-Checkpoints-9C276A.svg)]()
         modelscope
         github model zoos: [![github_model_zoos](https://img.shields.io/badge/ModelZoo-black?logo=github)]()
-->

<!-- Ê®°ÁâàÔºö
|** ** `Arxiv`|** ** [![](https://img.shields.io/badge/Github-181717?style=flat-square&logo=github&logoColor=white)](https://om-cat.github.io.)|unreleased|‚úì|‚úì|‚úì|‚úì|
`Arxiv`
![Stars](https://img.shields.io/github/stars/LaVi-Lab/Rethink_CoT_Video?style=flat-square&color=E0E0E0&label=Stars)
 ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square) 
 ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square)
 ![Audio](https://img.shields.io/badge/Audio-F2D1B3?style=flat-square)
 ![Speech](https://img.shields.io/badge/Speech-F4B6C2?style=flat-square)

![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square)
![Streaming](https://img.shields.io/badge/Streaming-F2D1B3?style=flat-square) 
![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square)
 
 -->

 
 ### üïπÔ∏è CoT-based Video Reasoning

| **Title** | **Model & Code** | **Checkpoint** | **Input Modalities** | **Time** | **Venue** |
|:----------|:-------------------:|:----------------:|:----------------------:|:----------:|:-----------:|
| [Rethinking Chain-of-Thought Reasoning for Videos](https://arxiv.org/abs/2512.09616) | [GitHub](https://github.com/LaVi-Lab/Rethink_CoT_Video) ![](https://img.shields.io/github/stars/LaVi-Lab/Rethink_CoT_Video?style=flat-square&color=E0E0E0&label=Stars) |`N/A`| ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square)|2025-12|`Arxiv`|
| [1+1 > 2 : Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673) | [GitHub](https://github.com/gaostar123/DeViL) ![](https://img.shields.io/github/stars/gaostar123/DeViL?style=flat-square&color=E0E0E0&label=Stars) |`N/A`| ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square)|2025-12|`Arxiv`|
| [TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning](https://arxiv.org/pdf/2512.03963) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) |2025-12|`Arxiv`|
| [OneThinker: All-in-one Reasoning Model for Image and Video](https://arxiv.org/abs/2512.03043) | [GitHub](https://github.com/tulerfeng/OneThinker) ![](https://img.shields.io/github/stars/tulerfeng/OneThinker?style=flat-square&color=E0E0E0&label=Stars) |[Hugging Face](https://huggingface.co/OneThink)| ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square)|2025-12|`Arxiv`|
| [WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning](https://arxiv.org/abs/2512.02425) | [GitHub](https://github.com/wgcyeo/WorldMM) ![](https://img.shields.io/github/stars/wgcyeo/WorldMM?style=flat-square&color=E0E0E0&label=Stars) | `N/A` | ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square)|2025-12|`Arxiv`
| [Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding](https://arxiv.org/abs/2512.00805) | `N/A` | `N/A`| ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square)|2025-12|`Arxiv`
| [Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models](https://arxiv.org/abs/2511.23478) | [GitHub](https://github.com/mbzuai-oryx/Video-R2) ![](https://img.shields.io/github/stars/mbzuai-oryx/Video-R2?style=flat-square&color=E0E0E0&label=Stars) |  `N/A` | ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-11| `Arxiv`
| [Video-CoM: Interactive Video Reasoning via Chain of Manipulations](https://arxiv.org/abs/2511.23477) | [GitHub](https://github.com/mbzuai-oryx/Video-CoM) ![](https://img.shields.io/github/stars/mbzuai-oryx/Video-CoM?style=flat-square&color=E0E0E0&label=Stars) |  `N/A` | ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square)  | 2025-11 |`Arxiv`|
| [VideoSeg-R1: Reasoning Video Object Segmentation via Reinforcement Learning](https://arxiv.org/abs/2511.16077)| [GitHub](https://github.com/euyis1019/VideoSeg-R1) ![](https://img.shields.io/github/stars/euyis1019/VideoSeg-R1?style=flat-square&color=E0E0E0&label=Stars)|  `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-11 |`Arxiv`|
| [AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning](https://arxiv.org/abs/2511.15578)| `N/A` | `N/A` | ![Audio](https://img.shields.io/badge/Audio-F2D1B3?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-11 |`Arxiv`|
| [Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding](https://arxiv.org/abs/2511.14446) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-11 |`Arxiv`|
| [Video Spatial Reasoning with Object-Centric 3D Rollout](https://arxiv.org/abs/2511.13190)| `N/A` | `N/A` | ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)   ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-11 |`Arxiv`|
| [ViSS-R1: Self-Supervised Reinforcement Video Reasoning](https://arxiv.org/abs/2511.13054) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-11 |`Arxiv`|
| [Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning](https://arxiv.org/abs/2510.23473)| [GitHub](https://github.com/shijian2001/Video-Thinker) ![](https://img.shields.io/github/stars/shijian2001/Video-Thinker?style=flat-square&color=E0E0E0&label=Stars)| [Hugging Face](https://huggingface.co/ShijianW01/Video-Thinker-7B) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-10 | `Arxiv` |
| [Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence](https://arxiv.org/pdf/2510.20579)| [GitHub](https://github.com/marinero4972/Open-o3-Video) ![](https://img.shields.io/github/stars/marinero4972/Open-o3-Video?style=flat-square&color=E0E0E0&label=Stars)| [Hugging Face](https://huggingface.co/marinero4972/Open-o3-Video) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-10 | `Arxiv` |
| [VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception](https://arxiv.org/abs/2509.21100) | [GitHub](https://github.com/OpenGVLab/VideoChat-R1) ![](https://img.shields.io/github/stars/OpenGVLab/VideoChat-R1?style=flat-square&color=E0E0E0&label=Stars)| [Hugging Face](https://huggingface.co/OpenGVLab/VideoChat-R1_5-7B) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-09 | `Arxiv` |
| [MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning](https://arxiv.org/abs/2509.21113) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-09 | `Arxiv` |
| [Kwai Keye-VL 1.5 Technical Report](https://arxiv.org/abs/2509.01563) | [GitHub](https://github.com/Kwai-Keye/Keye) ![](https://img.shields.io/github/stars/Kwai-Keye/Keye?style=flat-square&color=E0E0E0&label=Stars)| [Hugging Face](https://huggingface.co/Kwai-Keye)  |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-09 | `Arxiv` |
| [Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data](https://arxiv.org/abs/2509.03501) | [GitHub](https://github.com/SalesforceAIResearch/strefer) ![](https://img.shields.io/github/stars/SalesforceAIResearch/strefer?style=flat-square&color=E0E0E0&label=Stars)| [Google_Drive](https://drive.google.com/file/d/1hUa_A_qp7stsDhrRuD7_4NotAs6PW2gz/view?usp=sharing) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-09 | `Arxiv` |
| [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-08 | `Arxiv` |
| [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737) |  [GitHub](https://github.com/AIDC-AI/Ovis) ![](https://img.shields.io/github/stars/AIDC-AI/Ovis?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/collections/AIDC-AI/ovis25) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-08 | `Arxiv` |
| [Veason-R1: Reinforcing Video Reasoning Segmentation to Think Before It Segments](https://arxiv.org/abs/2508.11538) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-08 | `Arxiv` |
| [ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking](https://arxiv.org/abs/2508.05221) |  [GitHub](https://github.com/Event-AHU/Open_VLTrack) ![](https://img.shields.io/github/stars/Event-AHU/Open_VLTrack?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-08 | `Arxiv` |
| [TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding](https://arxiv.org/abs/2508.07683) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-08 | `Arxiv` |
| [Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning](https://arxiv.org/abs/2508.04416) |  [GitHub](https://github.com/zhang9302002/ThinkingWithVideos) ![](https://img.shields.io/github/stars/zhang9302002/ThinkingWithVideos?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/datasets/zhang9302002/MultiTaskVideoReasoning) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-08 | `Arxiv` |
| [AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video](https://arxiv.org/abs/2508.03100) |  [GitHub](https://github.com/yogkul2000/AVATAR) ![](https://img.shields.io/github/stars/yogkul2000/AVATAR?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/yogkul2000/AVATAR) | ![Audio](https://img.shields.io/badge/Audio-F2D1B3?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square)  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square) | 2025-08 | `Arxiv` |
| [ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models](https://arxiv.org/abs/2508.01533) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-08 | `Arxiv` |
| [VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering](https://arxiv.org/abs/2508.03039) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-08 |  `ACM-MM 2025` |
| [ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts](https://arxiv.org/abs/2507.20939) |  [GitHub](https://github.com/TencentARC/ARC-Hunyuan-Video-7B) ![](https://img.shields.io/github/stars/TencentARC/ARC-Hunyuan-Video-7B?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/TencentARC/ARC-Hunyuan-Video-7B) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square) ![Audio](https://img.shields.io/badge/Audio-F2D1B3?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-07 | `Arxiv` |
| [METER: Multi-modal Evidence-based Thinking and Explainable Reasoning -- Algorithm and Benchmark](https://arxiv.org/abs/2507.16206) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square) ![Audio](https://img.shields.io/badge/Audio-F2D1B3?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-07 | `Arxiv` |
| [CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks](https://arxiv.org/abs/2507.13609) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-07 | `Arxiv` |
| [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-07 | `Arxiv` |
| [Scaling RL to Long Videos](https://arxiv.org/abs/2507.07966) | [GitHub](https://github.com/NVlabs/Long-RL) ![](https://img.shields.io/github/stars/NVlabs/Long-RL?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/datasets/LongVideo-Reason/longvideo-reason) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-07 |  `NeurIPS 2025`  |
| [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949) |  [GitHub](https://github.com/Kwai-Keye/Keye) ![](https://img.shields.io/github/stars/Kwai-Keye/Keye?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-07 | `Arxiv` |
| [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876) |  [GitHub](https://github.com/BRZ911/ViTCoT) ![](https://img.shields.io/github/stars/BRZ911/ViTCoT?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-07 |  `ACM-MM 2025`|
| [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485) |  [GitHub](https://github.com/Ziyang412/Video-RTS) ![](https://img.shields.io/github/stars/Ziyang412/Video-RTS?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/Ted412/Video-RTS) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-07 |  `EMNLP 2025`|
| [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-07 | `Arxiv` |
| [VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.17221) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning](https://arxiv.org/abs/2506.13654) | [GitHub](https://github.com/egolife-ai/Ego-R1) ![](https://img.shields.io/github/stars/egolife-ai/Ego-R1?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 |  `Arxiv`|
| [DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning](https://arxiv.org/abs/2506.14827) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [VidBridge-R1: Bridging QA and Captioning for RL-based Video Understanding Models with Intermediate Proxy Tasks](https://arxiv.org/abs/2506.09079) | [GitHub](https://github.com/VidBridge-R1/VidBridge-R1) ![](https://img.shields.io/github/stars/VidBridge-R1/VidBridge-R1?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/datasets/VidBridge-R1/VidBridge-R1_training_data) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569) | [GitHub](https://github.com/XiaomiMiMo/MiMo-VL) ![](https://img.shields.io/github/stars/XiaomiMiMo/MiMo-VL?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/collections/XiaomiMiMo/mimo-vl) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning](https://arxiv.org/abs/2506.03525) | [GitHub](https://github.com/daeunni/Video-Skill-CoT) ![](https://img.shields.io/github/stars/daeunni/Video-Skill-CoT?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 |  `EMNLP 2025 (Findinds)` |
| [EgoVLM: Policy Optimization for Egocentric Video Understanding](https://arxiv.org/abs/2506.03097) | [GitHub](https://github.com/adityavavre/VidEgoVLM) ![](https://img.shields.io/github/stars/adityavavre/VidEgoVLM?style=flat-square&color=E0E0E0&label=Stars) |  [Hugging Face](https://huggingface.co/datasets/omlab/VLM-R1) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency](https://arxiv.org/abs/2506.01908) | [GitHub](https://github.com/appletea233/Temporal-R1) ![](https://img.shields.io/github/stars/appletea233/Temporal-R1?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/appletea2333/temporal-r1-7b-base) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking](https://arxiv.org/abs/2506.01725) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding](https://arxiv.org/abs/2506.01300) | [GitHub](https://github.com/aiming-lab/ReAgent-V) ![](https://img.shields.io/github/stars/aiming-lab/ReAgent-V?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 |  `NeurIPS 2025` |
| [ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding](https://arxiv.org/abs/2506.01274) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [DIVE: Deep-search Iterative Video Exploration](https://arxiv.org/abs/2506.21891) | [Github](https://github.com/PanasonicConnect/DIVE) ![](https://img.shields.io/github/stars/PanasonicConnect/DIVE?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `CVPR 2025` |
| [VideoDeepResearch: Long Video Understanding With Agentic Tool Using](https://arxiv.org/abs/2506.10821) | [Github](https://github.com/yhy-2000/VideoDeepResearch) ![](https://img.shields.io/github/stars/yhy-2000/VideoDeepResearch?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/abs/2506.07464) | [Github](https://github.com/mlvlab/DeepVideoR1) ![](https://img.shields.io/github/stars/mlvlab/DeepVideoR1?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 |  `NeurIPS 2025` |
| [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://dl.acm.org/doi/10.1145/3746027.3758313) | `N/A` | [Project_Page](https://video-CoT.github.io/) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning](https://arxiv.org/abs/2506.06097) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning](https://arxiv.org/abs/2506.00318) | [Github](https://github.com/SaraGhazanfari/CoF) ![](https://img.shields.io/github/stars/SaraGhazanfari/CoF?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/saraghznfri/CoF-models) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-06 | `Arxiv` |
| [Reinforcing Video Reasoning with Focused Thinking](https://arxiv.org/abs/2505.24718) |  [Github](https://github.com/longmalongma/TW-GRPO) ![](https://img.shields.io/github/stars/longmalongma/TW-GRPO?style=flat-square&color=E0E0E0&label=Stars)  | [Hugging Face](https://huggingface.co/Falconss1/TW-GRPO)|  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding](https://arxiv.org/abs/2505.21962) |  [Github](https://github.com/2-mo/A2Seek) ![](https://img.shields.io/github/stars/2-mo/A2Seek?style=flat-square&color=E0E0E0&label=Stars)  | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration](https://arxiv.org/abs/2505.20256) |  [Github](https://github.com/aim-uofa/Omni-R1) ![](https://img.shields.io/github/stars/aim-uofa/Omni-R1?style=flat-square&color=E0E0E0&label=Stars)  | [Hugging Face](https://huggingface.co/Haoz0206/Omni-R1) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square) ![Audio](https://img.shields.io/badge/Audio-F2D1B3?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `NeurIPS 2025` |
| [Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought](https://arxiv.org/abs/2505.19877) |  [Github](https://github.com/wbfwonderful/Vad-R1) ![](https://img.shields.io/github/stars/wbfwonderful/Vad-R1?style=flat-square&color=E0E0E0&label=Stars)  | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `NeurIPS 2025` |
| [VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Guided Iterative Policy Optimization](https://arxiv.org/abs/2505.19000) |  [Github](https://github.com/HITsz-TMG/VerIPO) ![](https://img.shields.io/github/stars/HITsz-TMG/VerIPO?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/Uni-MoE/VerIPO-7B-v1.0) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning](https://arxiv.org/abs/2505.16836) | [Github](https://github.com/zfr00/Fact-R1) ![](https://img.shields.io/github/stars/zfr00/Fact-R1?style=flat-square&color=E0E0E0&label=Stars) | `N/A`  |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Speech](https://img.shields.io/badge/Speech-F4B6C2?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 |  `NeurIPS 2025` |
| [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966) |  [Github](https://github.com/TIGER-AI-Lab/Pixel-Reasoner) ![](https://img.shields.io/github/stars/TIGER-AI-Lab/Pixel-Reasoner?style=flat-square&color=E0E0E0&label=Stars)  | [Hugging Face](https://huggingface.co/TIGER-Lab/PixelReasoner-RL-v1) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 |  `NeurIPS 2025` |
| [UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning](https://arxiv.org/abs/2505.14231) |  [Github](https://github.com/AMAP-ML/UniVG-R1) ![](https://img.shields.io/github/stars/AMAP-ML/UniVG-R1?style=flat-square&color=E0E0E0&label=Stars)  | [Hugging Face](https://huggingface.co/GD-ML/UniVG-R1) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning](https://arxiv.org/abs/2505.12434) |  [Github](https://github.com/QiWang98/VideoRFT) ![](https://img.shields.io/github/stars/QiWang98/VideoRFT?style=flat-square&color=E0E0E0&label=Stars) |  [Hugging Face](https://huggingface.co/QiWang98/VideoRFT) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `NeurIPS 2025` |
| [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action](https://arxiv.org/abs/2505.01583) |  [Github](https://github.com/Andy-Cheng/TEMPURA) ![](https://img.shields.io/github/stars/Andy-Cheng/TEMPURA?style=flat-square&color=E0E0E0&label=Stars)  | [Hugging Face](https://huggingface.co/andaba/TEMPURA-Qwen2.5-VL-3B-s1) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457) |  [Github](https://github.com/sail-sg/Video-Next-Event-Prediction) ![](https://img.shields.io/github/stars/sail-sg/Video-Next-Event-Prediction?style=flat-square&color=E0E0E0&label=Stars)  | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [SiLVR: A Simple Language-based Video Reasoning Framework](https://arxiv.org/abs/2505.24869) |  [Github](https://github.com/CeeZh/SILVR) ![](https://img.shields.io/github/stars/CeeZh/SILVR?style=flat-square&color=E0E0E0&label=Stars)  | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [RVTBench: A Benchmark for Visual Reasoning Tasks](https://arxiv.org/abs/2505.11838) | [GitHub](https://github.com/yiqings/rvt) ![](https://img.shields.io/github/stars/yiqings/rvt?style=flat-square&color=E0E0E0&label=Stars) |  [Hugging Face](https://huggingface.co/datasets/yiqingshen/rvtbench/tree/main/rvtbench) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [CoT-Vid: Dynamic Chain-of-Thought Routing with Self-Verification for Training-Free Video Reasoning](https://arxiv.org/abs/2505.11830) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models](https://arxiv.org/abs/2505.08455) | [GitHub](https://github.com/pritamqu/VCRBench) ![](https://img.shields.io/github/stars/pritamqu/VCRBench?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `Arxiv` |
| [AVA: Towards Agentic Video Analytics with Vision Language Models](https://arxiv.org/abs/2505.00254) | [GitHub](https://github.com/I-ESC/Project-Ava) ![](https://img.shields.io/github/stars/I-ESC/Project-Ava?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-05 | `NSDI 2026` |
| [TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning](https://arxiv.org/abs/2504.09641) |  [GitHub](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1) ![](https://img.shields.io/github/stars/ZhangXJ199/TinyLLaVA-Video-R1?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/Zhang199/TinyLLaVA-Video-R1)|  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-04 | `Arxiv` |
| [VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.06958) | [GitHub](https://github.com/OpenGVLab/VideoChat-R1) ![](https://img.shields.io/github/stars/OpenGVLab/VideoChat-R1?style=flat-square&color=E0E0E0&label=Stars)| [Hugging Face](https://huggingface.co/collections/OpenGVLab/videochat-r1) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-04 | `Arxiv` |
| [Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning](https://arxiv.org/abs/2504.01805) |  [GitHub](https://github.com/OuyangKun10/SpaceR) ![](https://img.shields.io/github/stars/OuyangKun10/SpaceR?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/datasets/RUBBISHLIKE/SpaceR-151k) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-04 | `Arxiv` |
| [Improved Visual-Spatial Reasoning via R1-Zero-Like Training](https://arxiv.org/abs/2504.00883) | [GitHub](https://github.com/zhijie-group/R1-Zero-VSI) ![](https://img.shields.io/github/stars/zhijie-group/R1-Zero-VSI?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/collections/nvidia/eagle) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-04 | `Arxiv` |
| [Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models](https://arxiv.org/abs/2504.15271) |  [GitHub](https://github.com/NVlabs/Eagle) ![](https://img.shields.io/github/stars/NVlabs/Eagle?style=flat-square&color=E0E0E0&label=Stars)  | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-04 | `Arxiv` |
| [LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding](https://arxiv.org/abs/2504.06835) | `N/A` |`N/A`|  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-04 | `Arxiv` |
| [From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models](https://arxiv.org/abs/2504.06214) | `N/A` |  [Hugging Face](https://huggingface.co/nvidia/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct)  |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-04 | `Arxiv` |
| [MR. Video: "MapReduce" is the Principle for Long Video Understanding](https://arxiv.org/abs/2504.16082) |  [GitHub](https://github.com/ziqipang/MR-Video) ![](https://img.shields.io/github/stars/ziqipang/MR-Video?style=flat-square&color=E0E0E0&label=Stars)  | `N/A`|  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-04 | `Arxiv` |
| [Multimodal Long Video Modeling Based on Temporal Dynamic Context](https://arxiv.org/abs/2504.10443) |  [GitHub](https://github.com/Hoar012/TDC-Video) ![](https://img.shields.io/github/stars/Hoar012/TDC-Video?style=flat-square&color=E0E0E0&label=Stars)  | [Hugging Face](https://huggingface.co/Hoar012/TDC-Qwen2-7B) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-04 | `Arxiv` |
| [WikiVideo: Article Generation from Multiple Videos](https://arxiv.org/abs/2504.00939) |  [GitHub](https://github.com/alexmartin1722/wikivideo) ![](https://img.shields.io/github/stars/alexmartin1722/wikivideo?style=flat-square&color=E0E0E0&label=Stars)  | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-04 | `Arxiv` |
| [Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](https://arxiv.org/abs/2503.24376) | [GitHub](https://github.com/TencentARC/SEED-Bench-R1) ![](https://img.shields.io/github/stars/TencentARC/SEED-Bench-R1?style=flat-square&color=E0E0E0&label=Stars) |  [Hugging Face](https://huggingface.co/datasets/TencentARC/SEED-Bench-R1) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-03 | `Arxiv` |
| [Video-R1: Reinforcing Video Reasoning in MLLMs](https://arxiv.org/abs/2503.21776) | [GitHub](https://github.com/tulerfeng/Video-R1) ![](https://img.shields.io/github/stars/tulerfeng/Video-R1?style=flat-square&color=E0E0E0&label=Stars)  |  [Hugging Face](https://huggingface.co/datasets/Video-R1/Video-R1-data) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-03 | `NeurIPS 2025` |
| [TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM](https://arxiv.org/abs/2503.13377) |[GitHub](https://github.com/xiaomi-research/time-r1) ![](https://img.shields.io/github/stars/xiaomi-research/time-r1?style=flat-square&color=E0E0E0&label=Stars)  | [Hugging Face](https://huggingface.co/Boshenxx/Time-R1-7B) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-03 | `NeurIPS 2025` |
| [ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos](https://arxiv.org/abs/2503.12542) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-03 | `NeurIPS 2025` |
| [VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning](https://arxiv.org/abs/2503.13444) | [GitHub](https://github.com/yeliudev/VideoMind) ![](https://img.shields.io/github/stars/yeliudev/VideoMind?style=flat-square&color=E0E0E0&label=Stars)  |  [Hugging Face](https://huggingface.co/collections/yeliudev/videomind) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-03 | `Arxiv` |
| [Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs](https://arxiv.org/abs/2503.23219) |[GitHub](https://github.com/schowdhury671/aurelia) ![](https://img.shields.io/github/stars/schowdhury671/aurelia?style=flat-square&color=E0E0E0&label=Stars)  | `N/A` | ![Audio](https://img.shields.io/badge/Audio-F2D1B3?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square)  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square) | 2025-03 | `ICCV 2025` |
| [video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model](https://arxiv.org/abs/2502.11775) | [GitHub](https://github.com/BriansIDP/video-SALMONN-o1) ![](https://img.shields.io/github/stars/BriansIDP/video-SALMONN-o1?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/tsinghua-ee/video-SALMONN-o1) | ![Audio](https://img.shields.io/badge/Audio-F2D1B3?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square)  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square) | 2025-02 | `Arxiv` |
| [TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding](https://arxiv.org/abs/2502.19400) |  [GitHub](https://github.com/TIGER-AI-Lab/TheoremExplainAgent) ![](https://img.shields.io/github/stars/TIGER-AI-Lab/TheoremExplainAgent?style=flat-square&color=E0E0E0&label=Stars) |  `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-02 |  `ACL 2025 (Oral)` |
| [CoS: Chain-of-Shot Prompting for Long Video Understanding](https://arxiv.org/abs/2502.06428) |[GitHub](https://github.com/lwpyh/CoS_codes) ![](https://img.shields.io/github/stars/lwpyh/CoS_codes?style=flat-square&color=E0E0E0&label=Stars) |  `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-02 | `Arxiv` |
| [Temporal Preference Optimization for Long-Form Video Understanding](https://arxiv.org/abs/2501.13919) |  [GitHub](https://github.com/ruili33/TPO) ![](https://img.shields.io/github/stars/ruili33/TPO?style=flat-square&color=E0E0E0&label=Stars) |  [Hugging Face](https://huggingface.co/collections/ruili0/temporal-preference-optimization) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-01 | `Arxiv` |
| [InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model](https://arxiv.org/abs/2501.12368) | [GitHub](https://github.com/InternLM/InternLM-XComposer) ![](https://img.shields.io/github/stars/InternLM/InternLM-XComposer?style=flat-square&color=E0E0E0&label=Stars) |  [Hugging Face](https://huggingface.co/internlm/internlm-xcomposer2d5-7b-reward) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-01 |  `ACL 2025 (Findings)` |
| [MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning](https://arxiv.org/abs/2501.07227) | [GitHub](https://github.com/tychen-SJTU/MECD-Benchmark) ![](https://img.shields.io/github/stars/tychen-SJTU/MECD-Benchmark?style=flat-square&color=E0E0E0&label=Stars) |  [Hugging Face](https://github.com/tychen-SJTU/MECD-Benchmark) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-01 |   TPAMI |
| [Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs](https://arxiv.org/abs/2501.04336) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-01 | `Arxiv` |
| [Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition](https://arxiv.org/abs/2501.03230) | [GitHub](https://github.com/scofield7419/Video-of-Thought) ![](https://img.shields.io/github/stars/scofield7419/Video-of-Thought?style=flat-square&color=E0E0E0&label=Stars)  | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2025-01 |  `ICML 2024` |
| [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://arxiv.org/abs/2412.05271) |  [GitHub](https://github.com/OpenGVLab/InternVL) ![](https://img.shields.io/github/stars/OpenGVLab/InternVL?style=flat-square&color=E0E0E0&label=Stars)  | [Hugging Face](https://github.com/OpenGVLab/InternVL) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2024-12 | `Arxiv` |
| [STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training](https://arxiv.org/abs/2412.00161) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2024-12 | `CVPR 2025` |
| [VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection](https://openaccess.thecvf.com/content/CVPR2025/html/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.html) | [GitHub](https://github.com/hshjerry/VideoEspresso) ![](https://img.shields.io/github/stars/hshjerry/VideoEspresso?style=flat-square&color=E0E0E0&label=Stars) |  [Hugging Face](https://huggingface.co/datasets/hshjerry0315/VideoEspresso-Test) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2024-11 | `CVPR 2025` |
| [Adaptive Video Understanding Agent: Enhancing efficiency with dynamic frame sampling and feedback-driven reasoning](https://arxiv.org/abs/2410.20252) | `N/A` | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2024-10 | `NeurIPS 2024 (Workshop)` |
| [VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs](https://arxiv.org/abs/2409.20365) | [GitHub](https://github.com/mayhugotong/VideoINSTA) ![](https://img.shields.io/github/stars/mayhugotong/VideoINSTA?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2024-09 | `EMNLP 2024 (Findinds)` |
| [MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning](https://arxiv.org/abs/2409.17647) | [GitHub](https://github.com/tychen-SJTU/MECD-Benchmark) ![](https://img.shields.io/github/stars/tychen-SJTU/MECD-Benchmark?style=flat-square&color=E0E0E0&label=Stars) |  [Hugging Face](https://github.com/tychen-SJTU/MECD-Benchmark) |  ![Text](https://img.shields.io/badge/Text-AEC6DF?style=flat-square)  ![Video](https://img.shields.io/badge/Video-C3E6CB?style=flat-square) | 2024-09 |  `NeurIPS 2024 (Spotlight)` |

### üïπÔ∏è CoF-based Video Reasoning
| **Title** | **Code** | **Checkpoint** | **Time** | **Venue** |
|:-----------|:------------------:|:----------------:|:----------:|:-----------:|
| [Unified Video Editing with Temporal Reasoner](https://arxiv.org/abs/2512.07469) | [GitHub](https://github.com/knightyxp/VideoCoF) ![](https://img.shields.io/github/stars/knightyxp/VideoCoF?style=flat-square&color=E0E0E0&label=Stars)|[Hugging Face](https://huggingface.co/XiangpengYang/VideoCoF)|2025-12| `Arxiv` |
| [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven‚Äô Matrices](https://arxiv.org/abs/2512.05969) | [GitHub](https://github.com/Video-Reason/VMEvalKit) ![](https://img.shields.io/github/stars/Video-Reason/VMEvalKit?style=flat-square&color=E0E0E0&label=Stars)| `N/A` |2025-12| `Arxiv` |
| [McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning](https://arxiv.org/abs/2511.22974) | [GitHub](https://github.com/QiushiYang/McSc) ![](https://img.shields.io/github/stars/QiushiYang/McSc?style=flat-square&color=E0E0E0&label=Stars)| `N/A` |2025-11| `Arxiv` |
| [In-Video Instructions: Visual Signals as Generative Control](https://arxiv.org/abs/2511.19401) | [GitHub](https://github.com/VainF/In-Video-Instructions) ![](https://img.shields.io/github/stars/VainF/In-Video-Instructions?style=flat-square&color=E0E0E0&label=Stars)| `N/A` |2025-11| `Arxiv` |
| [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/abs/2511.16669) | [GitHub](https://github.com/KlingTeam/VANS) ![](https://img.shields.io/github/stars/KlingTeam/VANS?style=flat-square&color=E0E0E0&label=Stars)|[Hugging Face](https://huggingface.co/KlingTeam/VANS)|2025-11| `Arxiv` |
| [Reasoning via Video: The First Evaluation of Video Models‚Äô Reasoning Abilities through Maze-Solving Tasks](https://arxiv.org/abs/2511.15065) | [GitHub](https://github.com/ImYangC7/VR-Bench) ![](https://img.shields.io/github/stars/ImYangC7/VR-Bench?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/HY-Wan/Wan-R1) | 2025-11 |`Arxiv`  |
| [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)| [GitHub](https://github.com/tongjingqi/Thinking-with-Video) ![](https://img.shields.io/github/stars/tongjingqi/Thinking-with-Video?style=flat-square&color=E0E0E0&label=Stars) | `N/A` | 2025-11| `Arxiv`|
| [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](https://arxiv.org/abs/2510.26802) | [GitHub](https://github.com/ZiyuGuo99/MME-CoF) ![](https://img.shields.io/github/stars/ZiyuGuo99/MME-CoF?style=flat-square&color=E0E0E0&label=Stars) |  [Hugging Face](https://huggingface.co/datasets/ZiyuG/MME-CoF) | 2025-10 | `Arxiv` | 
| [VChain : Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)|  [GitHub](https://github.com/Eyeline-Labs/VChain) ![](https://img.shields.io/github/stars/Eyeline-Labs/VChain?style=flat-square&color=E0E0E0&label=Stars) | `N/A` |2025-10|`Arxiv` |
| [Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning](https://arxiv.org/abs/2506.00318) | [GitHub](https://github.com/SaraGhazanfari/CoF) ![](https://img.shields.io/github/stars/SaraGhazanfari/CoF?style=flat-square&color=E0E0E0&label=Stars) | `N/A` | 2025-06 | `Arxiv` |



### üåà Interleaved Video Reasoning

| **Title** | **Code** |  **Checkpoint** | **Time** | **Venue** |
|:----------|:----------:|:--------------------:|:-----------:|:--:|
| [LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling](https://arxiv.org/abs/2511.20785)| [GitHub](https://github.com/EvolvingLMMs-Lab/LongVT) ![](https://img.shields.io/github/stars/EvolvingLMMs-Lab/LongVT?style=flat-square&color=E0E0E0&label=Stars)| [Hugging Face](https://huggingface.co/collections/lmms-lab/longvt) |2025-11|`Arxiv`|
| [JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation](https://javisverse.github.io/JavisGPT-page/) |[GitHub](https://github.com/JavisVerse/JavisGPT) ![](https://img.shields.io/github/stars/JavisVerse/JavisGPT?style=flat-square&color=E0E0E0&label=Stars)| `N/A` |2025-11| `NeurIPS 2025 (Spotlight)`|
| [Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination](https://arxiv.org/abs/2511.17490)| [GitHub](https://github.com/yunlong10/Video-R4) ![](https://img.shields.io/github/stars/yunlong10/Video-R4?style=flat-square&color=E0E0E0&label=Stars)| `N/A` | 2025-11 |`Arxiv`|
| [Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution](https://arxiv.org/abs/2511.14210) | `N/A` | `N/A` | 2025-11 | `Arxiv`  |
| [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022) | `N/A` | `N/A` | 2025-10 | `Arxiv` |
| [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://dl.acm.org/doi/abs/10.1145/3746027.3755837) |  [GitHub](https://github.com/BRZ911/ViTCoT) ![](https://img.shields.io/github/stars/BRZ911/ViTCoT?style=flat-square&color=E0E0E0&label=Stars)  | [Hugging Face](https://huggingface.co/datasets/BRZ911/ViTCoT)| 2025-10 | `ACM-MM 2025`|
| [FrameMind: Frame-Interleaved Video Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.24008) | `N/A` | `N/A`   |2025-09 | `Arxiv`  |
| [Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning](https://arxiv.org/abs/2508.04416) |  [GitHub](https://github.com/zhang9302002/ThinkingWithVideos) ![](https://img.shields.io/github/stars/zhang9302002/ThinkingWithVideos?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/datasets/zhang9302002/MultiTaskVideoReasoning) | 2025-08 | `Arxiv` |
| [VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://arxiv.org/abs/2409.04429) | [GitHub](https://github.com/mit-han-lab/vila-u) ![](https://img.shields.io/github/stars/mit-han-lab/vila-u?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/collections/mit-han-lab/vila-u-7b)   |2024-09 | `ICLR 2025`|


### üîÅ Streaming Video Reasoning

| **Title** | **Code** |  **Checkpoint** | **Time** | **Venue** |
|:----------|:----------:|:----------:|:----------:|:-----------:|
| [LiveStar: Live Streaming Assistant for Real-World Online Video Understanding](https://arxiv.org/abs/2511.05299) | [GitHub](https://github.com/sotayang/LiveStar) ![](https://img.shields.io/github/stars/sotayang/LiveStar?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/yzy666/LiveStar_8B) | 2025-11 |  `NeurIPS 2025` |
| [StreamingVLM: Real-Time Understanding for Infinite Video Streams](https://arxiv.org/abs/2510.09608) | [GitHub](https://github.com/mit-han-lab/streaming-vlm) ![](https://img.shields.io/github/stars/mit-han-lab/streaming-vlm?style=flat-square&color=E0E0E0&label=Stars) | `N/A` | 2025-10 | `Arxiv` |
| [StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding](https://arxiv.org/abs/2508.01875) | `N/A` | `N/A` | 2025-10 | `Arxiv` |
| [StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA](https://arxiv.org/pdf/2510.25332) |[GitHub](https://github.com/Fleeting-hyh/StreamingCoT?tab=readme-ov-file#minerva) ![](https://img.shields.io/github/stars/Fleeting-hyh/StreamingCoT?style=flat-square&color=E0E0E0&label=Stars) | `N/A` | 2025-10 |`ACM-MM 2025`|
| [StreamForest: Efficient Online Video Understanding with Persistent Event Memory](https://arxiv.org/abs/2509.24871) |  [GitHub](https://github.com/MCG-NJU/StreamForest) ![](https://img.shields.io/github/stars/MCG-NJU/StreamForest?style=flat-square&color=E0E0E0&label=Stars)  | [Hugging Face](https://huggingface.co/collections/MCG-NJU/streamforest-and-odvbench) | 2025-09 | `NeurIPS 2025 (Spotlighht)` |
| [StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling](https://arxiv.org/abs/2507.05240) | [GitHub](https://github.com/InternRobotics/StreamVLN) ![](https://img.shields.io/github/stars/InternRobotics/StreamVLN?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/mengwei0427/StreamVLN_Video_qwen_1_5_r2r_rxr_envdrop_scalevln_v1_3) |  2025-07 | `Arxiv` |
| [Flash-VStream: Efficient Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2506.23825) | [GitHub](https://github.com/IVGSZ/Flash-VStream) ![](https://img.shields.io/github/stars/IVGSZ/Flash-VStream?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/zhang9302002/Flash-VStream-Qwen-7b) | 2025-06 | `ICCV 2025`  |
| [StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant](https://arxiv.org/abs/2505.05467) | [GitHub](https://github.com/apple/ml-streambridge) ![](https://img.shields.io/github/stars/apple/ml-streambridge?style=flat-square&color=E0E0E0&label=Stars) | `N/A` | 2025-05 |`NeurIPS 2025`  |
| [LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval](https://arxiv.org/abs/2505.15269) | `N/A` | `N/A` | 2025-05 | `Arxiv` |
| [TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/abs/2504.17343) | [GitHub](https://github.com/yaolinli/TimeChat-Online) ![](https://img.shields.io/github/stars/yaolinli/TimeChat-Online?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/wyccccc/TimeChatOnline-7B) | 2025-04 |  `ACM-MM 2025` |
| [LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale](https://arxiv.org/abs/2504.16030) | [GitHub](https://github.com/showlab/livecc) ![](https://img.shields.io/github/stars/showlab/livecc?style=flat-square&color=E0E0E0&label=Stars) | `N/A` | 2025-04 | `Arxiv` |
| [ViSpeak: Visual Instruction Feedback in Streaming Videos](https://arxiv.org/abs/2503.12769) | [GitHub](https://github.com/HumanMLLM/ViSpeak) ![](https://img.shields.io/github/stars/HumanMLLM/ViSpeak?style=flat-square&color=E0E0E0&label=Stars) | [Model_Zoo](https://github.com/HumanMLLM/ViSpeak#-model-zoo) | 2025-03 | `ICCV 2025`  |
| [StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through Event-Gated Cognition](https://arxiv.org/abs/2503.06220) | [GitHub](https://github.com/xinding-sys/StreamMind) ![](https://img.shields.io/github/stars/xinding-sys/StreamMind?style=flat-square&color=E0E0E0&label=Stars) | `N/A` | 2025-03 |`ICCV 2025` |
| [Streaming Video Question-Answering with In-context Video KV-Cache Retrieval](https://arxiv.org/abs/2503.00540) | [GitHub](https://github.com/Becomebright/ReKV) ![](https://img.shields.io/github/stars/Becomebright/ReKV?style=flat-square&color=E0E0E0&label=Stars) | `N/A` | 2025-03 | `ICLR 2025` |
| [SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding](https://arxiv.org/abs/2502.10810) | [GitHub](https://github.com/sotayang/SVBench) ![](https://img.shields.io/github/stars/sotayang/SVBench?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/yzy666/StreamingChat_8B) | 2025-02 | `ICLR 2025 (Spotlight)` |
| [Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction](https://arxiv.org/abs/2501.03218) | [GitHub](https://github.com/Mark12Ding/Dispider) ![](https://img.shields.io/github/stars/Mark12Ding/Dispider?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/Mar2Ding/Dispider) | 2025-01 | `CVPR 2025` |
| [Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge](https://arxiv.org/abs/2501.13468) |  [GitHub](https://github.com/hmxiong/StreamChat) ![](https://img.shields.io/github/stars/hmxiong/StreamChat?style=flat-square&color=E0E0E0&label=Stars)  | `N/A` |  2025-01 | `ICLR 2025` |
| [Online Video Understanding: A Comprehensive Benchmark and Memory-Augmented Method](https://arxiv.org/abs/2501.00584) |  [GitHub](https://github.com/MCG-NJU/VideoChat-Online) ![](https://img.shields.io/github/stars/MCG-NJU/VideoChat-Online?style=flat-square&color=E0E0E0&label=Stars) | [Hugging Face](https://huggingface.co/datasets/MCG-NJU/OVBench) | 2025-01 | `CVPR 2025` |
| [StreamChat: Chatting with Streaming Video](https://arxiv.org/abs/2412.08646) | `N/A` |`N/A` | 2024-11 | `Arxiv` |




## ‚ú®Ô∏è Benchmarks

| **Name**      | **Paper** | **Link** | **Task** | **Time** | **Venue** |
|:--------------------:|:-----------|:-------------:|:----------:|:----------:|:-----------:|
| MMGR |[MMGR: Multi-Modal Generative Reasoning](https://arxiv.org/pdf/2512.14691)| [GitHub](https://github.com/Zefan-Cai/MMGR) ![](https://img.shields.io/github/stars/Zefan-Cai/MMGR?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/ZefanCai/Video-Reasoning-Clean)  |![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square)|2015-12| `Arxiv`|
| MM-CoT |[MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models](https://arxiv.org/pdf/2512.08228)|`N/A` |![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square)|2015-12| `Arxiv`|
| RULER-Bench |[RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence](https://arxiv.org/abs/2512.02622) |  [GitHub](https://github.com/hexmSeeU/RULER-Bench) ![](https://img.shields.io/github/stars/hexmSeeU/RULER-Bench?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/hexmSeeU/RULER-Bench) |![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square) |2025-12| `Arxiv` |
| AV-SpeakerBench |[See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2512.02231) |  [GitHub](https://github.com/plnguyen2908/AV-SpeakerBench) ![](https://img.shields.io/github/stars/plnguyen2908/AV-SpeakerBench?style=flat-square&color=E0E0E0&label=Stars) |![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) |2025-12| `Arxiv` |
| PAI-Bench | [PAI-Bench: A Comprehensive Benchmark For Physical AI](https://arxiv.org/abs/2512.01989) |  [GitHub](https://github.com/SHI-Labs/physical-ai-bench) ![](https://img.shields.io/github/stars/SHI-Labs/physical-ai-bench?style=flat-square&color=E0E0E0&label=Stars) |![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) ![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square) |2025-12| `Arxiv` |
| Envision | [Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights](https://arxiv.org/abs/2512.01816) |  [GitHub](https://github.com/opendatalab-raiser/Envision) ![](https://img.shields.io/github/stars/opendatalab-raiser/Envision?style=flat-square&color=E0E0E0&label=Stars) |![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square) |2025-12| `Arxiv` |
| STREAMGAZE | [STREAMGAZE: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos](https://arxiv.org/abs/2512.01707) |  [GitHub](https://github.com/daeunni/StreamGaze) ![](https://img.shields.io/github/stars/daeunni/StreamGaze?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/danaleee/StreamGaze) |![Streaming](https://img.shields.io/badge/Streaming-F2D1B3?style=flat-square) ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) |2025-12| `Arxiv` |
| V-ReasonBench | [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](https://arxiv.org/abs/2511.16668) | [GitHub](https://github.com/yangluo7/V-ReasonBench) ![](https://img.shields.io/github/stars/yangluo7/V-ReasonBench?style=flat-square&color=E0E0E0&label=Stars) | ![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square) | 2025-11 | `Arxiv` |
| VR-Bench|[Reasoning via Video: The First Evaluation of Video Models‚Äô Reasoning Abilities through Maze-Solving Tasks](https://arxiv.org/abs/2511.15065) | [GitHub](https://github.com/ImYangC7/VR-Bench) ![](https://img.shields.io/github/stars/ImYangC7/VR-Bench?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/amagipeng/VR-Bench) | ![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square) |2025-11 |`Arxiv`  |
| Gen-ViRe   | [Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark](https://arxiv.org/abs/2511.13853) | [GitHub](https://github.com/L-CodingSpace/GVR) ![](https://img.shields.io/github/stars/L-CodingSpace/GVR?style=flat-square&color=E0E0E0&label=Stars)| ![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square) | 2025-11 |`Arxiv`|
| TiViBench | [TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models](https://arxiv.org/abs/2511.13704) | [GitHub](https://github.com/EnVision-Research/TiViBench) ![](https://img.shields.io/github/stars/EnVision-Research/TiViBench?style=flat-square&color=E0E0E0&label=Stars) | ![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square) | 2025-11 | `Arxiv` |
| VideoThinkBench |[Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)| [GitHub](https://github.com/tongjingqi/Thinking-with-Video) ![](https://img.shields.io/github/stars/tongjingqi/Thinking-with-Video?style=flat-square&color=E0E0E0&label=Stars) | ![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square) | 2025-11| `Arxiv`|
| MME-CoF | [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](https://arxiv.org/abs/2510.26802) | [Hugging Face](https://huggingface.co/datasets/ZiyuG/MME-CoF) | ![Vision](https://img.shields.io/badge/Vision-C3E6CB?style=flat-square) | 2025-10 | `Arxiv` |
| SciVideoBench | [SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models](https://arxiv.org/abs/2510.08559) | [GitHub](https://github.com/dengandong/SciVideoBench) ![](https://img.shields.io/github/stars/dengandong/SciVideoBench?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-10 | `Arxiv`  |
| ReasoningTrack | [ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking](https://arxiv.org/abs/2508.05221) | [GitHub](https://github.com/Event-AHU/Open_VLTrack)![](https://img.shields.io/github/stars/Event-AHU/Open_VLTrack?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-08 | `Arxiv` |
| METER | [METER: Multi-modal Evidence-based Thinking and Explainable Reasoning -- Algorithm and Benchmark](https://arxiv.org/abs/2507.16206) | `N/A` | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-07 | `Arxiv` |
| Video-TT | [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028) | [Hugging Face](https://huggingface.co/datasets/lmms-lab/video-tt) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-07 |  `ICCV 2025` |
| ImplicitQA | [ImplicitQA: Going beyond frames towards Implicit Video Reasoning](https://arxiv.org/abs/2506.21742) | [Hugging Face](https://huggingface.co/datasets/ucf-crcv/ImplicitQA) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-06 | `Arxiv` |
| Video-CoT | [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/abs/2506.08817) | [Hugging Face](https://huggingface.co/datasets/Zooy138/Video-CoT) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-06 | `Arxiv` |
| Implicit-VideoQA | [Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning](https://arxiv.org/abs/2506.07811) | [GitHub](https://github.com/tychen-SJTU/Implicit-VideoQA)![](https://img.shields.io/github/stars/tychen-SJTU/Implicit-VideoQA?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-06 | `Arxiv` |
| MORSE-500 | [MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning](https://arxiv.org/abs/2506.05523) | [GitHub](https://github.com/morse-benchmark/morse-500) ![](https://img.shields.io/github/stars/morse-benchmark/morse-500?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/video-reasoning/morse-500) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-06 | `Arxiv` |
| SpookyBench | [Time Blindness: Why Video-Language Models Can't See What Humans Can](https://arxiv.org/abs/2505.24867) | [GitHub](https://github.com/TimeBlindness/time-blindness) ![](https://img.shields.io/github/stars/TimeBlindness/time-blindness?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/timeblindness/spooky-bench) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-05 | `Arxiv` |
| VideoReasonBench|[VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?](https://arxiv.org/abs/2505.23359) |  [GitHub](https://github.com/llyx97/video_reason_bench) ![](https://img.shields.io/github/stars/llyx97/video_reason_bench?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/lyx97/reasoning_videos) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-05 | `Arxiv` |
| Video-Holmes | [Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?](https://arxiv.org/abs/2505.21374) | [GitHub](https://github.com/TencentARC/Video-Holmes) ![](https://img.shields.io/github/stars/TencentARC/Video-Holmes?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-05 | `Arxiv` |
| VideoEval-Pro | [VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation](https://arxiv.org/abs/2505.14640) | [GitHub](https://github.com/TIGER-AI-Lab/VideoEval-Pro) ![](https://img.shields.io/github/stars/TIGER-AI-Lab/VideoEval-Pro?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-05 | `Arxiv` |
| VBenchComp | [Breaking Down Video LLM Benchmarks](https://arxiv.org/abs/2505.14321) | `N/A` | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-05 | `Arxiv` |
| RVTBench|[RVTBench: A Benchmark for Visual Reasoning Tasks](https://arxiv.org/abs/2505.11838) |  [GitHub](https://github.com/yiqings/rvt) ![](https://img.shields.io/github/stars/yiqings/rvt?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/yiqingshen/rvtbench/tree/main/rvtbench) |![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-05 | `Arxiv` |
| VCRBench |[VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models](https://arxiv.org/abs/2505.08455) | [GitHub](https://github.com/pritamqu/VCRBench) ![](https://img.shields.io/github/stars/pritamqu/VCRBench?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-05 | `Arxiv` |
| RTV-Bench | [RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video](https://arxiv.org/abs/2505.02064) | [GitHub](https://github.com/LJungang/RTV-Bench) ![](https://img.shields.io/github/stars/LJungang/RTV-Bench?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/RTVBench/RTV-Bench) |![Streaming](https://img.shields.io/badge/Streaming-F2D1B3?style=flat-square) ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-05 | `NeurIPS 2025 (D&B)` |
| MINERVA | [MINERVA: Evaluating Complex Video Reasoning](https://arxiv.org/abs/2505.00681) | [GitHub](https://github.com/google-deepmind/neptune?tab=readme-ov-file#minerva) ![](https://img.shields.io/github/stars/google-deepmind/neptune?style=flat-square&color=E0E0E0&label=Stars)| ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-05 | `Arxiv` |
| VCR-Bench | [VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.07956) | [GitHub](https://github.com/zhishuifeiqian/VCR-Bench) ![](https://img.shields.io/github/stars/zhishuifeiqian/VCR-Bench?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/VLM-Reasoning/VCR-Bench) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) |2025-04 | `Arxiv` |
| SEED-Bench-R1 | [Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](https://arxiv.org/abs/2503.24376) | [GitHub](https://github.com/TencentARC/SEED-Bench-R1) ![](https://img.shields.io/github/stars/TencentARC/SEED-Bench-R1?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/TencentARC/SEED-Bench-R1) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) |2025-03 | `Arxiv` |
| H2VU-Benchmark | [H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding](https://arxiv.org/abs/2503.24008) | [GitHub](https://github.com/siriusrecco/H2VU-BenchMark) ![](https://img.shields.io/github/stars/siriusrecco/H2VU-BenchMark?style=flat-square&color=E0E0E0&label=Stars) | ![Streaming](https://img.shields.io/badge/Streaming-F2D1B3?style=flat-square) ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-03 | `Arxiv` |
| OmniMMI | [OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts](https://arxiv.org/abs/2503.22952) | [GitHub](https://github.com/OmniMMI/OmniMMI) ![](https://img.shields.io/github/stars/OmniMMI/OmniMMI?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/ColorfulAI/OmniMMI) |![Streaming](https://img.shields.io/badge/Streaming-F2D1B3?style=flat-square) ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) |2025-03 | `CVPR 2025` |
| HAVEN | [Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation](https://arxiv.org/abs/2503.19622) | [GitHub](https://github.com/Hongcheng-Gao/HAVEN) ![](https://img.shields.io/github/stars/Hongcheng-Gao/HAVEN?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://github.com/Hongcheng-Gao/HAVEN/blob/main/Data/test_data.json) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) |2025-03 | `Arxiv` |
| V-STaR | [V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning](https://arxiv.org/abs/2503.11495) | [GitHub](https://github.com/V-STaR-Bench/V-STaR) ![](https://img.shields.io/github/stars/V-STaR-Bench/V-STaR?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/V-STaR-Bench/V-STaR) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) |2025-03 | `Arxiv` |
| COVER | [Reasoning is All You Need for Video Generalization](https://arxiv.org/abs/2503.10691) | [GitHub](https://github.com/gongyifan-hash/COVER-Benchmark) ![](https://img.shields.io/github/stars/gongyifan-hash/COVER-Benchmark?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-03 | `ACL 2025 (Findinds)` |
| MOMA-QA | [Towards Fine-Grained Video Question Answering](https://arxiv.org/abs/2503.06820) | `N/A` | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-03 |  `Arxiv` |
| SVBench | [SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding](https://arxiv.org/abs/2502.10810) |  [GitHub](https://github.com/sotayang/SVBench) ![](https://img.shields.io/github/stars/sotayang/SVBench?style=flat-square&color=E0E0E0&label=Stars) | ![Streaming](https://img.shields.io/badge/Streaming-F2D1B3?style=flat-square) ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-02 | `ICLR 2025 (Spotlight)` |
| StreamBench | [Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge](https://arxiv.org/abs/2501.13468) | [GitHub](https://github.com/hmxiong/StreamChat) ![](https://img.shields.io/github/stars/hmxiong/StreamChat?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/Barry-12138/StreamBench_v0.3) |![Streaming](https://img.shields.io/badge/Streaming-F2D1B3?style=flat-square) ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) |2025-01 | `ICLR 2025` |
| MMVU | [MMVU: Measuring Expert-Level Multi-Discipline Video Understanding](https://arxiv.org/abs/2501.12380) | [GitHub](https://github.com/yale-nlp/MMVU) ![](https://img.shields.io/github/stars/yale-nlp/MMVU?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/yale-nlp/MMVU) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) |2025-01 | `Arxiv` |
| OVO-Bench | [OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?](https://arxiv.org/abs/2501.05510) | [GitHub](https://github.com/JoeLeelyf/OVO-Bench) ![](https://img.shields.io/github/stars/JoeLeelyf/OVO-Bench?style=flat-square&color=E0E0E0&label=Stars)[Hugging Face](https://huggingface.co/datasets/JoeLeelyf/OVO-Bench) |![Streaming](https://img.shields.io/badge/Streaming-F2D1B3?style=flat-square) ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-01 | `CVPR 2025` |
| HLV-1K | [HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding](https://arxiv.org/abs/2501.01645) | [GitHub](https://github.com/Vincent-ZHQ/HLV-1K-Long-Video-Understanding-Benchmark) ![](https://img.shields.io/github/stars/Vincent-ZHQ/HLV-1K-Long-Video-Understanding-Benchmark?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-01 | `ICME 2025` |
| OVBench | [Online Video Understanding: OVBench and VideoChat-Online](https://arxiv.org/abs/2501.00584) | [GitHub](https://github.com/MCG-NJU/VideoChat-Online) ![](https://img.shields.io/github/stars/MCG-NJU/VideoChat-Online?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/datasets/MCG-NJU/OVBench) |![Streaming](https://img.shields.io/badge/Streaming-F2D1B3?style=flat-square) ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2025-01 | `CVPR 2025` |
| VSI-Bench | [Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces](https://arxiv.org/abs/2412.14171) | [GitHub](https://github.com/vision-x-nyu/thinking-in-space) ![](https://img.shields.io/github/stars/vision-x-nyu/thinking-in-space?style=flat-square&color=E0E0E0&label=Stars)| ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2024-12 | `CVPR 2025 (Oral)` |
| 3DSRBench | [3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark](https://arxiv.org/abs/2412.07825) | [Hugging Face](https://huggingface.co/datasets/ccvl/3DSRBench) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2024-12 | `ICCV 2025`  |
| BlackSwanSuite | [Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events](https://arxiv.org/abs/2412.05725) | [GitHub](https://github.com/sahithyaravi/BlackSwan) ![](https://img.shields.io/github/stars/sahithyaravi/BlackSwan?style=flat-square&color=E0E0E0&label=Stars)<br>[Hugging Face](https://huggingface.co/collections/UBC-ViL/black-swan-abductive-and-defeasible-reasoning-67de1a4ab7ddc22edf0b0542) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square)|2024-12 | `CVPR 2025` |
| TOMATO | [TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models](https://proceedings.iclr.cc/paper_files/paper/2025/hash/16ba99f25a235f1100a4014d71d34ad8-Abstract-Conference.html) | [Github](https://github.com/yale-nlp/TOMATO) ![](https://img.shields.io/github/stars/yale-nlp/TOMATO?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2024-10 | `CVPR 2025` |
| OmnixR    | [OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities](https://proceedings.iclr.cc/paper_files/paper/2025/hash/aa3e67220ca4cd50010165c950fc8056-Abstract-Conference.html) | `N/A` | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2024-10 |  `ICLR 2025` |
| VideoVista | [VideoVista: A Versatile Benchmark for Video Understanding and Reasoning](https://arxiv.org/abs/2406.11303) | [Github](https://github.com/HITsz-TMG/Uni-MoE/tree/master/VideoVista) ![](https://img.shields.io/github/stars/HITsz-TMG/Uni-MoE?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2024-06 |  `Arxiv` |
| SOK-Bench | [SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge](https://arxiv.org/abs/2405.09713) | [GitHub](https://github.com/csbobby/SOK-Bench) ![](https://img.shields.io/github/stars/csbobby/SOK-Bench?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2024-05 |   `CVPR 2024` |
| CVRR-ES | [How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs](https://arxiv.org/abs/2405.03690) |[GitHub](https://github.com/mbzuai-oryx/CVRR-Evaluation-Suite/) ![](https://img.shields.io/github/stars/mbzuai-oryx/CVRR-Evaluation-Suite?style=flat-square&color=E0E0E0&label=Stars) | ![Language](https://img.shields.io/badge/Language-AEC6DF?style=flat-square) | 2024-05 |  `Arxiv` |


## ‚úà Related Survey

In addition, several recent and concurrent surveys have discussed multimodal or video reasoning. The works listed below offer complementary perspectives to ours, reflecting the field‚Äôs rapid and parallel development:
- [Awesome-Video-Reasoning](https://github.com/Video-Reason/Awesome-Video-Reasoning)
- [Awesome-LLMs-for-Video-Understanding](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding)
- [Awesome-Multimodal-Spatial-Reasoning](https://github.com/zhengxuJosh/Awesome-Multimodal-Spatial-Reasoning)
- [Awesome-Streaming-Video-Understanding](https://github.com/sotayang/Awesome-Streaming-Video-Understanding)
- [Awesome-Video-LMM-Post-Training](https://github.com/yunlong10/Awesome-Video-LMM-Post-Training)
---
- [Awesome-World-Models](https://github.com/knightnemo/Awesome-World-Models)
- [Awesome-World-Models-for-Robotics](https://github.com/leofan90/Awesome-World-Models)
---
- [Awesome-MCoT](https://github.com/yaotingwangofficial/Awesome-MCoT)
- [Awesome-Latent-CoT](https://github.com/EIT-NLP/Awesome-Latent-CoT)
- [Awesome-Latent-Space](https://github.com/YU-deep/Awesome-Latent-Space)


## üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=LJungang/Awesome-Video-Reasoning-Landscape&type=Date)](https://star-history.com/#LJungang/Awesome-Omni-Large-Models-and-Datasets&Date)

## ‚ô•Ô∏è Contributors

<!--
<a href="https://github.com/LJungang/Awesome-Video-Reasoning-Landscape/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=LJungang/Awesome-Video-Reasoning-Landscape" />
</a>
 -->

<a href="https://github.comLJungang/Awesome-Video-Reasoning-Landscape/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=LJungang/Awesome-Video-Reasoning-Landscape" alt="Contributors for Awesome Video Reasoning Landscape"/>
</a>

<!-- markdownlint-enable MD033 -->
